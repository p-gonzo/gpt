{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dea4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace, Split\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad9f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE vocabulary\n",
    "vocab_size = 10000\n",
    "min_frequency=2\n",
    "#hyperparameters\n",
    "batch_size = 64 # independent sequences processed in parallel\n",
    "block_size = 256 #max context length for predictions\n",
    "step = 0\n",
    "eval_interval = 500\n",
    "learning_rate = 5e-4\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0f21b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212a8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('allLyrics.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f2bf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(BPE())\n",
    "# tokenizer.pre_tokenizer = Split(Regex('[ \\t]+'), behavior='removed')\n",
    "# trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency)\n",
    "# tokenizer.train(files=['allLyrics.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b45f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9334d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "encode = lambda s: tokenizer.encode(s).ids\n",
    "decode = lambda l: tokenizer.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b005e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "All of this and more is for you\n",
      "With love, sincerity and deepest care\n",
      "My life with you I share\n",
      "\n",
      "\n",
      "Ever since I met you, baby\n",
      "I've been wantin' to lay you down\n",
      "But it's so hard to get you\n",
      "Baby, when you never come around\n",
      "Every day that you keep it a\n",
      "---\n",
      "250\n",
      "61\n",
      "[0, 0, 0, 450, 198, 282, 179, 406, 177, 231, 347, 595, 832, 8548, 1771, 868, 179, 1169, 782, 2670, 413, 440, 251, 162, 40, 7609, 0, 0, 3598, 1391, 40, 1411, 512, 600, 539, 443, 9560, 170, 897, 162, 611, 295, 383, 266, 615, 170, 256, 347, 1137, 325, 162, 374, 336, 1212, 526, 339, 202, 162, 501, 168, 62]\n"
     ]
    }
   ],
   "source": [
    "y = text[:250]\n",
    "print(y)\n",
    "print(\"---\")\n",
    "print(len(y))\n",
    "x = encode(y)\n",
    "print(len(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57893ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77584d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " All of this and more is for you\n",
      " With love, sin cer ity and deep est care\n",
      " My life with you I share\n",
      " \n",
      " \n",
      " Ever since I met you, baby\n",
      " I've been wantin' to lay you down\n",
      " But it's so hard to get you\n",
      " Baby, when you never come around\n",
      " Every day that you keep it a\n"
     ]
    }
   ],
   "source": [
    "print(decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f46530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s]\n",
    "# decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16de0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4607af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d18b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2d50ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        #compute attn scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92058a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of attention in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2438e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear leayer followed by a non-linearity\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9716a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "    \n",
    "    def __init__(self, nembd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: th number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de5033d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #number of embedding dimensions\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # postion of each char\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # language model head\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of ints\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb= self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled indext to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc4deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e716ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b76b37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(str(pathlib.Path().resolve()) + '/snapshot5')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "step = checkpoint['step']\n",
    "losses = checkpoint['losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59b68c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11000: train loss 0.8441, val loss 8.2380\n",
      "Step 11500: train loss 0.8107, val loss 8.3545\n",
      "Step 12000: train loss 0.7773, val loss 8.4241\n",
      "Step 12500: train loss 0.7373, val loss 8.4931\n"
     ]
    }
   ],
   "source": [
    "for itr in range(2000):\n",
    "    # every once in a while evaluate the loss on the train and val sets:\n",
    "    if itr % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f7f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a74a0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " I’ve been so long\n",
      " Re mem ' the sun rise has been rob ins up\n",
      " Then they laid Him in free ?\n",
      " \n",
      " \n",
      " Come along and take an empt iness you've been little girl, my mind cha long ed windows , didn't have I can ever known\n",
      " I can make it with you\n",
      " 4 a choice heart sing\n",
      " These are sad and sad and the pink Cadillac\n",
      " Cr own the sun 's grown\n",
      " He's all going to all around life in his eyes\n",
      " \n",
      " Come here and take some bur g\n",
      " K new s, yeah\n",
      " I, I am our to meet again moon up now, happy let them and so I\n",
      " Let's do this, stop shiver what you want to\n",
      " I, I just don't even think about\n",
      " All I say\n",
      " \n",
      " Boys in ta this in ju ic al, drink s, ru b\n",
      " I think it's lost and clear through a bas ket ball of diamonds in teeth\n",
      " Oh, can you go out of the blue always\n",
      " Yes, out of my life\n",
      " I can't go to fire runnin' away I ain't enough\n",
      " I go hard again.\n",
      " \n",
      " It's like a long time\n",
      " I've been a long time\n",
      " Since I've seen a long time\n",
      " I've seen but I keep saying it's not too long\n",
      " How do you know why, yeah, you know just how to play this just lead Dear me.\n",
      " I just a prayer\n",
      " I just wanted to hold you 'round it in for\n",
      " With only that, before ... Oh, whatever you need\n",
      " I just miss get go get get do, say have found try need\n",
      " mean, love feel care do\n",
      " have think miss know tell know do, ask miss miss want write miss did miss needed thought take feel get am go found feel found be, miss get just let let find needed love go really miss let go miss wanna wish wish miss miss say hold feel care let want get said did\n",
      " make would want miss miss know miss miss miss write just feel said get miss miss get\n",
      " wish said love miss really tell want miss have let say tell ever say want be, just do, needed go been wanna say have miss miss do, need don't need say say feel miss let get hope been needed wished feel let found need wake feel said wish let hold do feel just need hold had miss feel just miss needed feel wanna need sh let get got get should miss have need miss miss need have don't give feel do, let miss needed don't miss write needed miss said miss needed miss just feel said wish still have miss just never find was said think get\n",
      " needed don't miss say have admit never have admit just feel miss love don't have need do miss feel say have say go need miss just go need feel miss needed feel tried been wished let needed knew go wanna feel miss tell don't tell needed hold miss give have\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cee2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#             'step': step,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'losses': losses,\n",
    "#             }, str(pathlib.Path().resolve()) + '/snapshot5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d85c97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save(\"tokenizer5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da04047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
